{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from tfidf import TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_dir = 'E:\\Masters\\Search Engines and Information Retrieval Systems\\Project\\podcasts-no-audio-13GB\\spotify-podcasts-2020\\podcasts-transcripts'\n",
    "rss_dir = 'E:\\Masters\\Search Engines and Information Retrieval Systems\\Project\\podcasts-no-audio-13GB\\spotify-podcasts-2020\\show-rss'\n",
    "\n",
    "trans2ID = {}\n",
    "transcriptLst = []\n",
    "idx = 0\n",
    "# Get all the 2 min window range\n",
    "# res -> [(show_filename_prefix, episode_filename_prefix)]\n",
    "# expandedQuery: The query modified with relevance feedback or k-gram\n",
    "# expandedQuery-> [{term1: score1}, ..., {termx: scorex}]\n",
    "# e.g. res= [(query, \"show_01DbRiALDPdvZdoiY8yQL6\", \"5fG4VlWnWwzAt6mSs0H7lY\", episcore)]\n",
    "\n",
    "table = TfIdf()\n",
    "\n",
    "class transcript():\n",
    "    def __init__(self, id, score, words):\n",
    "        self.id = id\n",
    "        self.score = score\n",
    "        self.words = words\n",
    "\n",
    "def getClips(res, n_minute, n_clips):\n",
    "    result = []\n",
    "    episode_frequency = {}\n",
    "    for (qr, show, episode, score) in res:\n",
    "        episode_frequency[episode] = 0\n",
    "        subdir1, subdir2 = show.split('_')[1][0], show.split('_')[1][1]\n",
    "        transcript_json_dir = os.path.join(transcript_dir, subdir1, subdir2, show, episode+'.json')\n",
    "        rss_xml_dir = os.path.join(rss_dir, subdir1, subdir2, show+'.xml')\n",
    "        rss_tree = ET.parse(rss_xml_dir)\n",
    "        rss_root = rss_tree.getroot()\n",
    "        rss_url = getUrlFromXml(rss_root)\n",
    "        with open(transcript_json_dir, ) as f:\n",
    "            transcript_json = json.load(f)\n",
    "            \n",
    "            addDocuments(transcript_json, score, show, episode, rss_url)\n",
    "    getRelevantClips(qr)\n",
    "    \n",
    "    rankingTranscript()\n",
    "    # for element in transcriptLst:\n",
    "    #     print(element.id, element.score)\n",
    "    for element in transcriptLst:\n",
    "        show, episode, rss_url, start, end = trans2ID[element.id]\n",
    "        cnt = element.id\n",
    "        if element.score == 0:\n",
    "            continue\n",
    "        while cnt + 1 < idx:\n",
    "            if trans2ID[cnt + 1][1] != episode or (trans2ID[cnt + 1][-1] - start) > n_minute or episode_frequency[episode] > n_clips:\n",
    "                break\n",
    "            end = trans2ID[cnt + 1][-1]\n",
    "            cnt += 1\n",
    "        result.append((show, episode, rss_url, start, end))\n",
    "        episode_frequency[episode] += 1\n",
    "    return result\n",
    "\n",
    "def getUrlFromXml(root):\n",
    "    item = root[0].find('item')\n",
    "    url = item.find('enclosure')\n",
    "    return url.attrib['url']\n",
    "\n",
    "def rankingTranscript():\n",
    "    transcriptLst.sort(key=lambda x: x.score, reverse=True)\n",
    "\n",
    "def addDocuments(data, score, show, episode, rss_url):\n",
    "    start = 0\n",
    "    end = 0\n",
    "    global idx\n",
    "    alternatives = data['results']\n",
    "    for alternative in alternatives:\n",
    "        transcripts_data = alternative['alternatives']\n",
    "        if transcripts_data == [{}]:\n",
    "            continue\n",
    "        for ele in transcripts_data:\n",
    "            if 'transcript' not in ele.keys():\n",
    "                continue\n",
    "            trans = ele['transcript']\n",
    "            start = float(ele['words'][0]['startTime'].rstrip('s'))\n",
    "            end = float(ele['words'][-1]['endTime'].rstrip('s'))\n",
    "            tmp = transcript(idx, score, trans.split())\n",
    "            # s = tmp.calTFIDF(qr)\n",
    "            # if s != 0:\n",
    "            transcriptLst.append(tmp)\n",
    "            table.add_document(idx, trans.upper().split())\n",
    "            trans2ID[idx] = (show, episode, rss_url, start, end)\n",
    "            \n",
    "            idx += 1\n",
    "    # print(table.corpus_dict)\n",
    "\n",
    "def getRelevantClips(qr, epi_w=0.4):\n",
    "    table2 = table.similarities(qr)\n",
    "    for i in range(idx):\n",
    "        transcriptLst[i].score = epi_w * transcriptLst[i].score + (1 - epi_w) * table2[i][1]\n",
    "    # print(table2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticSearchClient:\n",
    "    def __init__(self):\n",
    "        \n",
    "        print(\"Connecting to elastic search\")\n",
    "        self.client = Elasticsearch(['http://localhost:9200'], http_auth=('elastic', 'Xh2IngoELD1kH30khiKF'))\n",
    "        if self.client.ping():\n",
    "            print('Successfully connected to elasticsearch')\n",
    "        else:\n",
    "            print('Connection failed!')\n",
    "            return\n",
    "        \n",
    "        #self.client.indices.delete(index='spotify-metadata', ignore=[400, 404])   \n",
    "        if self.client.indices.exists(index = \"spotify-metadata\"):\n",
    "            print(\"Index already exists!\")\n",
    "        else:\n",
    "            print(\"Creating the metadata index...\")\n",
    "            response = self.create_index_metadata(indexname = \"spotify-metadata\")\n",
    "            print ('response:', response)\n",
    "        \n",
    "            print(\"Indexing documents\")\n",
    "\n",
    "            self.generate_actions_metadata(indexname = \"spotify-metadata\", metadatafile = \"podcasts-no-audio-13GB\\spotify-podcasts-2020\\metadata.tsv\")\n",
    "            \n",
    "        \n",
    "    def generate_actions_metadata(self, indexname, metadatafile):\n",
    "\n",
    "        doc_id_counter = 0\n",
    "        tsv_file = open(metadatafile, encoding=\"utf8\")\n",
    "        read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "        doc = {}\n",
    "        header = True\n",
    "        for row in read_tsv:\n",
    "            start = time.time()\n",
    "            if header:\n",
    "                header = False\n",
    "                continue\n",
    "            doc[\"show_uri\"] = row[0]\n",
    "            doc[\"show_name\"] = row[1]\n",
    "            doc[\"show_description\"] = row[2]\n",
    "            doc[\"publisher\"] = row[3]\n",
    "            doc[\"language\"] = row[4]\n",
    "            doc[\"rss_link\"] = row[5]\n",
    "            doc[\"episode_uri\"] = row[6]\n",
    "            doc[\"episode_name\"] = row[7]\n",
    "            doc[\"episode_description\"] = row[8]\n",
    "            doc[\"duration\"] = row[9]\n",
    "            doc[\"show_filename_prefix\"] = row[10]\n",
    "            doc[\"episode_filename_prefix\"] = row[11]\n",
    "\n",
    "            response = self.client.index(index=indexname, id=doc_id_counter, document=doc)\n",
    "            doc_id_counter += 1\n",
    "\n",
    "            #if (doc_id_counter%10000 == 0):\n",
    "            print(\"lines processed:\",doc_id_counter)\n",
    "            print(\"Time required : \", (time.time()-start))\n",
    "        tsv_file.close()\n",
    "    \n",
    "    \n",
    "    def create_index_metadata(self, indexname):\n",
    "        \n",
    "         response = self.client.indices.create(\n",
    "            index=indexname,\n",
    "            body={\n",
    "                \"settings\": {\"number_of_shards\": 2,\n",
    "                            \"number_of_replicas\": 1},\n",
    "                \"mappings\": {\n",
    "                    \"properties\": {\n",
    "                        \"show_uri\": {\"type\": \"keyword\"},\n",
    "                        \"show_name\": {\"type\": \"text\"},\n",
    "                        \"show_description\": {\"type\": \"text\"},\n",
    "                        \"publisher\": {\"type\": \"text\"},\n",
    "                        \"language\" : {\"type\": \"text\"},\n",
    "                        \"rss_link\" :{\"type\": \"text\"},\n",
    "                        \"episode_uri\" : {\"type\": \"keyword\"},\n",
    "                        \"show_name\" : {\"type\": \"text\"},\n",
    "                        \"episode_description\" : {\"type\": \"text\"},\n",
    "                        \"duration\" : {\"type\": \"float\"},\n",
    "                        \"show_filename_prefix\" : {\"type\": \"text\"},\n",
    "                        \"episode_filename_prefix\" : {\"type\": \"text\"}\n",
    "                    }\n",
    "                },\n",
    "            },\n",
    "            ignore=400,\n",
    "        )\n",
    "\n",
    "    \n",
    "    def search(self, query, fields=None):\n",
    "        query_body = {}\n",
    "        if fields == None:\n",
    "            query_body = {\n",
    "                \"query\": {\"multi_match\": {\n",
    "                    \"query\": query\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            query_body = {\n",
    "                \"query\": {\"multi_match\": {\n",
    "                    \"query\": query,\n",
    "                    \"fields\": fields\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        res = self.client.search(index=\"spotify-metadata\", body=query_body, size=100)\n",
    "        print(len(res['hits']['hits']))\n",
    "        result = []\n",
    "        for doc in res['hits']['hits']:\n",
    "            result.append((query.split(), doc['_source']['show_filename_prefix'], doc['_source']['episode_filename_prefix'], doc['_score']))\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    es = ElasticSearchClient()\n",
    "    start = time.time()\n",
    "    results = es.search(\"JESUS\")\n",
    "    result = getClips(results, 120, 2)\n",
    "    print(\"Time to search: \",(time.time()-start))\n",
    "    print(len(result))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to elastic search\n",
      "Successfully connected to elasticsearch\n",
      "Index already exists!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: The 'http_auth' parameter is deprecated. Use 'basic_auth' or 'bearer_auth' parameters instead\n",
      "  \"\"\"\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:104: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Time to search:  2.8095574378967285\n",
      "13442\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
